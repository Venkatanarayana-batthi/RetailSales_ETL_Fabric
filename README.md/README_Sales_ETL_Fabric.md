
# ðŸ’¼ Microsoft Fabric Project â€“ Sales ETL Pipeline

This project demonstrates an **end-to-end ETL pipeline** using **Microsoft Fabric**, featuring:
- ðŸ’¾ Data ingestion from CSVs
- ðŸ” Lookup of product categories
- ðŸ” Iterative transformation using **ForEach** and **Notebook**
- ðŸ§ª Delta table output for downstream analytics

---

## ðŸ“ Project Structure

```
Sales_ETL_Pipeline_Project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ products.csv
â”‚   â””â”€â”€ sales_data.csv
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ transform_sales_data.ipynb
â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ Sales_ETL_Pipeline (JSON files exported from Fabric)
â””â”€â”€ README.md
```

---

## ðŸ§  Key Components

### 1. ðŸ“„ Source Data

* **products.csv** â€“ Contains product details and category info  
* **sales_data.csv** â€“ Contains sales transactions  

These files are stored under:  
`RetailSalesLakehouse > Files > data/`

---

### 2. ðŸ“˜ Notebook: `transform_sales_data`

This notebook is executed dynamically for each category. It:

* Accepts a **category name** from the pipeline
* Reads both CSVs
* Filters sales by category
* Joins product details
* Calculates revenue
* Writes output as Delta table to:  
  `Tables > curated > sales_by_category`

#### ðŸ”§ Key code from notebook:

```python
category = spark.conf.get("category", "Electronics")
sales_df = spark.read.option("header", True).csv("Files/data/sales_data.csv")
products_df = spark.read.option("header", True).csv("Files/data/products.csv")
joined_df = sales_df.join(products_df, on="product_id", how="inner")
filtered_df = joined_df.filter(joined_df["category"] == category)
from pyspark.sql.functions import col
result_df = filtered_df.withColumn("revenue", col("quantity") * col("unit_price"))
result_df.write.mode("overwrite").format("delta").save("Tables/curated/sales_by_category")
```

---

### 3. ðŸ” Pipeline Design

#### âœ… Lookup: `LookupCategories`

* Type: File  
* Path: `Files/data/products.csv`  
* Reads distinct product categories from CSV

#### ðŸ” ForEach: `ForEachCategory`

* Sequential execution  
* Iterates over output from Lookup  
* Runs the notebook for each category

#### â–¶ Activity: `RunNotebookForCategory`

* Executes notebook with parameter:

```json
{
  "category": "@item().category"
}
```

---

## ðŸ–¼ Screenshots

### ðŸ” Lookup Activity Setup

![Lookup](screenshots/lookup-setup.png)

### ðŸ” ForEach Setup

![ForEach](screenshots/foreach-setup.png)

### ðŸ“˜ Notebook Code

![Notebook](screenshots/notebook-code.png)

---

## âœ… Output

Output is stored as **Delta table** under:  
`RetailSalesLakehouse > Tables > curated > sales_by_category`

This table is ready for **Power BI** or **Direct Lake queries**.

---

## ðŸš€ How to Run

1. Upload `products.csv` and `sales_data.csv` to:  
   `Files > data/`
2. Import the notebook and set up the connection to the Lakehouse.
3. Import and open the pipeline.
4. Validate â†’ Run the pipeline.

---

## ðŸ“¦ Deployment

To deploy this project:

* Clone the repo  
* Import pipeline JSON via Fabric UI  
* Update connections to your own Lakehouse  
* Run the pipeline

---

## ðŸ“Œ Tools Used

* **Microsoft Fabric**
* **Data Factory Pipelines**
* **Notebooks (PySpark)**
* **Delta Lake Tables**

---

